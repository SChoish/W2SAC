#!/usr/bin/env python3
"""
SAC with Multi-step Actor Update Training Script
"""

import argparse
import csv
import datetime
import gymnasium as gym
import gymnasium_robotics  # Adroit ë° Kitchen í™˜ê²½ ë“±ë¡ì„ ìœ„í•´ í•„ìš”
import itertools
import json
import numpy as np
import os
import torch
from pathlib import Path
from torch.utils.tensorboard import SummaryWriter

# í™˜ê²½ ë“±ë¡
gym.register_envs(gymnasium_robotics)

from sac import SAC
from replay_memory import ReplayMemory


# ---------------------------
# ìœ í‹¸ í•¨ìˆ˜
# ---------------------------
def set_global_seed(env, seed: int):
    """í™˜ê²½ ë° ëœë¤ ì‹œë“œ ì„¤ì • (POGO ìŠ¤íƒ€ì¼)"""
    import random
    try:
        env.reset(seed=seed)
    except TypeError:
        # old gym ë²„ì „
        env.seed(seed)
    try:
        env.action_space.seed(seed)
    except Exception:
        pass
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# ---------------------------
# í‰ê°€ ë£¨í‹´
# ---------------------------
@torch.no_grad()
def eval_policy(agent, eval_env, base_seed, eval_episodes=10, deterministic=True, actor_idx=None):
    """ì •ì±… í‰ê°€: deterministicê³¼ stochastic ëª¨ë‘ í‰ê°€ (POGO ìŠ¤íƒ€ì¼)
    
    Args:
        agent: SAC agent
        eval_env: í‰ê°€ìš© í™˜ê²½
        base_seed: ê¸°ë³¸ ì‹œë“œ
        eval_episodes: í‰ê°€ episode ìˆ˜
        deterministic: Trueë©´ deterministic í‰ê°€, Falseë©´ stochastic í‰ê°€
        actor_idx: í‰ê°€í•  actor ì¸ë±ìŠ¤ (Noneì´ë©´ ë§ˆì§€ë§‰ actor)
    
    Returns:
        tuple: (avg_reward, episode_rewards)
    """
    episode_rewards = []
    step_count = 0
    
    for ep in range(eval_episodes):
        # ì¬í˜„ì„±ì„ ìœ„í•´ ê° episodeë§ˆë‹¤ í™˜ê²½ ì‹œë“œ ì¬ì„¤ì • (POGO ìŠ¤íƒ€ì¼)
        ep_seed = base_seed + ep
        try:
            eval_env.action_space.seed(ep_seed)
            reset_result = eval_env.reset(seed=ep_seed)
        except (TypeError, AttributeError):
            try:
                reset_result = eval_env.reset(seed=ep_seed)
            except TypeError:
                reset_result = eval_env.reset()
        
        state = reset_result[0] if isinstance(reset_result, tuple) else reset_result
        # Dict observation space ì²˜ë¦¬
        if isinstance(state, dict):
            state = state['observation']
        
        episode_reward = 0
        done = False
        
        while not done:
            # ì¬í˜„ì„±ì„ ìœ„í•´ episodeì™€ step ê¸°ë°˜ ì‹œë“œ ì‚¬ìš© (POGO ìŠ¤íƒ€ì¼)
            action_seed = base_seed * 10000 + ep * 1000 + step_count if not deterministic else None
            action = agent.select_action(state, evaluate=deterministic, actor_idx=actor_idx, seed=action_seed)
            next_state, reward, terminated, truncated, _ = eval_env.step(action)
            done = terminated or truncated
            episode_reward += reward
            
            # Dict observation space ì²˜ë¦¬
            if isinstance(next_state, dict):
                next_state = next_state['observation']
            
            state = next_state
            step_count += 1
        
        episode_rewards.append(episode_reward)
    
    avg_reward = np.mean(episode_rewards)
    return avg_reward, episode_rewards


def final_evaluation(agent, env_name, seed, runs=5, episodes=10, actor_idx=None):
    """ìµœì¢… í‰ê°€: ì—¬ëŸ¬ runì— ê±¸ì³ í‰ê°€"""
    eval_env = gym.make(env_name)
    set_global_seed(eval_env, seed + 10_000)
    
    det_scores, stoch_scores = [], []
    for r in range(runs):
        det_avg, _ = eval_policy(
            agent, eval_env, base_seed=1000 + 100 * r, 
            eval_episodes=episodes, deterministic=True, actor_idx=actor_idx
        )
        stoch_avg, _ = eval_policy(
            agent, eval_env, base_seed=2000 + 100 * r, 
            eval_episodes=episodes, deterministic=False, actor_idx=actor_idx
        )
        det_scores.append(det_avg)
        stoch_scores.append(stoch_avg)
    
    det_scores = np.array(det_scores, dtype=np.float32)
    stoch_scores = np.array(stoch_scores, dtype=np.float32)
    
    print("======== Final Evaluation ========")
    print(f"[FINAL] Deterministic: mean={det_scores.mean():.3f}, std={det_scores.std():.3f} over {runs}x{episodes}")
    print(f"[FINAL] Stochastic:   mean={stoch_scores.mean():.3f}, std={stoch_scores.std():.3f} over {runs}x{episodes}")
    
    eval_env.close()
    return det_scores, stoch_scores


# ---------------------------
# ì²´í¬í¬ì¸íŠ¸ ìœ í‹¸
# ---------------------------
def save_checkpoint(agent, env_name: str, ckpt_dir: str, prefix: str, step: int, extra_meta=None):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    os.makedirs(ckpt_dir, exist_ok=True)
    path = os.path.join(ckpt_dir, prefix)
    agent.save_checkpoint(env_name, suffix=prefix, ckpt_path=path)
    
    meta = {
        "step": int(step),
        "checkpoint_name": prefix,
    }
    if extra_meta:
        meta.update(extra_meta)
    
    with open(path + "_meta.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)
    print(f"[CKPT] Saved: {path} (step={step})")


# ---------------------------
# í†µí•© í•™ìŠµ
# ---------------------------
def train_unified(agent, env, env_name, seed, memory, max_steps, eval_freq, 
                  updates_per_step, start_steps, batch_size, 
                  file_name, ckpt_dir, results_dir, writer, start_step=0):
    """í†µí•© í•™ìŠµ: ëª¨ë“  actorë¥¼ ë™ì‹œì— í•™ìŠµí•˜ê³  í‰ê°€"""
    
    eval_env = gym.make(env_name)
    # í‰ê°€ í™˜ê²½ì€ ë³„ë„ ì‹œë“œ ì‚¬ìš© (í•™ìŠµ í™˜ê²½ê³¼ ë¶„ë¦¬)
    eval_env_seed = seed + 1234
    set_global_seed(eval_env, eval_env_seed)
    
    num_actors = getattr(agent, "num_actors", 1)
    w2_weights = getattr(agent, "w2_weights", [0.0] * num_actors)
    w2_str = ", ".join([f"{w:.3f}" for w in w2_weights])
    
    print(f"ğŸš€ í†µí•© í•™ìŠµ ì‹œì‘: {start_step} ~ {max_steps-1} steps (SAC, {num_actors}ê°œ actor)")
    print(f"   Actor weights: [{w2_str}]")
    
    # í‰ê°€ ê²°ê³¼ ì €ì¥ìš©
    eval_files = {}
    evaluations = {}
    for i in range(num_actors):
        eval_file = results_dir / f"{file_name}_actor_{i}.npy"
        eval_files[i] = eval_file
        if start_step > 0 and eval_file.exists():
            evaluations[i] = list(np.load(eval_file))
        else:
            evaluations[i] = []
    
    # Metrics CSV íŒŒì¼
    log_dir = results_dir / "training"
    log_dir.mkdir(parents=True, exist_ok=True)
    metrics_file = log_dir / f"{file_name}_metrics.csv"
    metrics_file_exists = metrics_file.exists()
    
    if start_step == 0 or not metrics_file_exists:
        metrics_file_handle = open(metrics_file, 'w', newline='', encoding='utf-8')
        metrics_writer = None
    else:
        metrics_file_handle = open(metrics_file, 'a', newline='', encoding='utf-8')
        metrics_writer = None
    
    # í‰ê°€ ë¡œê·¸ íŒŒì¼
    eval_log_file = log_dir / f"{file_name}_evaluation.log"
    eval_log_handle = open(eval_log_file, 'a', encoding='utf-8') if eval_log_file.exists() else open(eval_log_file, 'w', encoding='utf-8')
    
    # ì´ì „ stepì˜ metrics ì €ì¥
    prev_metrics = {}
    
    # Training loop
    total_numsteps = start_step
    updates = 0
    episode_rewards = []
    
    for i_episode in itertools.count(1):
        episode_reward = 0
        episode_steps = 0
        done = False
        # ì¬í˜„ì„±ì„ ìœ„í•´ episodeë³„ë¡œ ë‹¤ë¥¸ ì‹œë“œ ì‚¬ìš© (POGO ìŠ¤íƒ€ì¼)
        episode_seed = seed + i_episode
        try:
            state, _ = env.reset(seed=episode_seed)
        except TypeError:
            state, _ = env.reset()
        try:
            env.action_space.seed(episode_seed)
        except Exception:
            pass
        
        # Dict observation space ì²˜ë¦¬
        if isinstance(state, dict):
            state = state['observation']
        
        while not done:
            if start_steps > total_numsteps:
                # ëœë¤ ì•¡ì…˜ ìƒ˜í”Œë§ ì‹œ ì‹œë“œ ê³ ì • (POGO ìŠ¤íƒ€ì¼)
                np.random.seed(episode_seed * 10000 + episode_steps)
                action = env.action_space.sample()
            else:
                # í•™ìŠµ ì¤‘ action ì„ íƒ ì‹œ ì‹œë“œ ê³ ì • (POGO ìŠ¤íƒ€ì¼: episodeì™€ step ê¸°ë°˜)
                action_seed = episode_seed * 10000 + episode_steps
                action = agent.select_action(state, evaluate=False, actor_idx=0, seed=action_seed)
            
            if len(memory) > batch_size:
                for _ in range(updates_per_step):
                    metrics = agent.update_parameters(memory, batch_size, updates)
                    
                    # TensorBoard ë¡œê¹… (ëª¨ë“  ë©”íŠ¸ë¦­)
                    writer.add_scalar('loss/critic_1', metrics['critic_1_loss'], updates)
                    writer.add_scalar('loss/critic_2', metrics['critic_2_loss'], updates)
                    writer.add_scalar('loss/policy', metrics['policy_loss'], updates)
                    writer.add_scalar('loss/entropy_loss', metrics['entropy_loss'], updates)
                    writer.add_scalar('entropy_temprature/alpha', metrics['alpha'], updates)
                    
                    # TensorBoard ë¡œê¹… (ìƒì„¸ ë©”íŠ¸ë¦­ - POGO ìŠ¤íƒ€ì¼)
                    for key, value in metrics.items():
                        if 'actor_' in key or 'Q_' in key or 'w2_' in key:
                            writer.add_scalar(f'metrics/{key}', value, updates)
                    
                    # í˜„ì¬ metricsì™€ ì´ì „ metrics ë³‘í•©
                    merged_metrics = prev_metrics.copy()
                    merged_metrics.update(metrics)
                    prev_metrics = merged_metrics.copy()
                    
                    # CSV ë¡œê¹…ì€ í‰ê°€ ì‹œì ì—ë§Œ ìˆ˜í–‰ (ì•„ë˜ í‰ê°€ ë¸”ë¡ì—ì„œ)
                    
                    updates += 1
            
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            episode_steps += 1
            total_numsteps += 1
            episode_reward += reward
            
            # Dict observation space ì²˜ë¦¬
            if isinstance(next_state, dict):
                next_state_obs = next_state['observation']
            else:
                next_state_obs = next_state
            
            mask = 0.0 if done else 1.0
            memory.push(state, action, reward, next_state_obs, mask)
            state = next_state_obs
        
        if total_numsteps > max_steps:
            break
        
        episode_rewards.append(episode_reward)
        writer.add_scalar('reward/train', episode_reward, i_episode)
        print(f"Episode: {i_episode}, total numsteps: {total_numsteps}, episode steps: {episode_steps}, reward: {episode_reward:.2f}")
        
        # í‰ê°€ (eval_freq ì£¼ê¸°ë¡œ ì‹¤í–‰)
        if i_episode % eval_freq == 0:
            print(f"\n[Evaluation] Episode: {i_episode}, Time steps: {total_numsteps}")
            
            # ëª¨ë“  actor í‰ê°€
            actor_results = []
            eval_metrics = {}  # í‰ê°€ ë©”íŠ¸ë¦­ ì €ì¥ìš©
            for i in range(num_actors):
                det_avg, _ = eval_policy(
                    agent, eval_env, base_seed=100 + i * 100, 
                    eval_episodes=10, deterministic=True, actor_idx=i
                )
                stoch_avg, _ = eval_policy(
                    agent, eval_env, base_seed=200 + i * 100, 
                    eval_episodes=10, deterministic=False, actor_idx=i
                )
                
                actor_results.append({
                    'det_avg': det_avg,
                    'stoch_avg': stoch_avg
                })
                
                # í‰ê°€ ë©”íŠ¸ë¦­ ì €ì¥ (CSVì— í¬í•¨ë  ë©”íŠ¸ë¦­)
                eval_metrics[f'actor_{i}_det_reward'] = det_avg
                eval_metrics[f'actor_{i}_sto_reward'] = stoch_avg
                
                # í‰ê°€ ê²°ê³¼ ì €ì¥
                evaluations[i].append(det_avg)
                np.save(eval_files[i], evaluations[i])
                
                # TensorBoard ë¡œê¹…
                writer.add_scalar(f'avg_reward/actor_{i}_deterministic', det_avg, i_episode)
                writer.add_scalar(f'avg_reward/actor_{i}_stochastic', stoch_avg, i_episode)
                writer.add_scalar(f'avg_reward/actor_{i}_deterministic_steps', det_avg, total_numsteps)
                writer.add_scalar(f'avg_reward/actor_{i}_stochastic_steps', stoch_avg, total_numsteps)
            
            # í‰ê°€ ë©”íŠ¸ë¦­ì„ prev_metricsì— ì¶”ê°€ (CSV ì €ì¥ìš©)
            prev_metrics.update(eval_metrics)
            
            # CSV ë¡œê¹… (í‰ê°€ ì‹œì ì— ëª¨ë“  ë©”íŠ¸ë¦­ ì €ì¥)
            row = {'step': total_numsteps, 'episode': i_episode}
            row.update(prev_metrics)  # ëª¨ë“  ë©”íŠ¸ë¦­ í¬í•¨ (í•™ìŠµ ë©”íŠ¸ë¦­ + í‰ê°€ ë©”íŠ¸ë¦­)
            
            if metrics_writer is None:
                fieldnames = ['step', 'episode'] + sorted([k for k in row.keys() if k not in ['step', 'episode']])
                metrics_writer = csv.DictWriter(metrics_file_handle, fieldnames=fieldnames, extrasaction='ignore')
                if not metrics_file_exists:
                    metrics_writer.writeheader()
            else:
                new_fields = [k for k in row.keys() if k not in metrics_writer.fieldnames]
                if new_fields:
                    metrics_writer.fieldnames = list(metrics_writer.fieldnames) + sorted(new_fields)
            
            metrics_writer.writerow(row)
            metrics_file_handle.flush()
            
            # ê²°ê³¼ ì¶œë ¥ (ë” ëª…í™•í•˜ê²Œ)
            print("=" * 60)
            print(f"Evaluation Results (Episode {i_episode}, Steps {total_numsteps}):")
            print("-" * 60)
            for i in range(num_actors):
                r = actor_results[i]
                print(f"  Actor {i} (w2_weight={w2_weights[i]:.3f}):")
                print(f"    Deterministic: {r['det_avg']:.2f}")
                print(f"    Stochastic:   {r['stoch_avg']:.2f}")
            print("=" * 60)
            print()
            
            # ë¡œì»¬ ë¡œê·¸ íŒŒì¼ì— ì €ì¥ (POGO ìŠ¤íƒ€ì¼)
            timestamp_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            eval_log_handle.write(f"[{timestamp_str}] Episode {i_episode}, Steps {total_numsteps}\n")
            eval_log_handle.write("-" * 60 + "\n")
            for i in range(num_actors):
                r = actor_results[i]
                eval_log_handle.write(f"Actor {i} (w2_weight={w2_weights[i]:.3f}):\n")
                eval_log_handle.write(f"  Deterministic: {r['det_avg']:.2f}\n")
                eval_log_handle.write(f"  Stochastic:   {r['stoch_avg']:.2f}\n")
            eval_log_handle.write("=" * 60 + "\n\n")
            eval_log_handle.flush()
    
    # Metrics íŒŒì¼ ë‹«ê¸°
    if metrics_file_handle is not None:
        metrics_file_handle.close()
        print(f"ğŸ“Š Metrics saved to: {metrics_file}")
    
    # í‰ê°€ ë¡œê·¸ íŒŒì¼ ë‹«ê¸°
    if eval_log_handle is not None:
        eval_log_handle.close()
        print(f"ğŸ“ Evaluation log saved to: {eval_log_file}")
    
    eval_env.close()
    return agent, episode_rewards


# ---------------------------
# Main
# ---------------------------
def parse_args():
    parser = argparse.ArgumentParser(description='PyTorch Soft Actor-Critic Args')
    
    # Environment
    parser.add_argument('--env-name', default="Humanoid-v5",
                        help='Mujoco Gym environment (default: Humanoid-v5)')
    parser.add_argument('--seed', type=int, default=123456, metavar='N',
                        help='random seed (default: 123456)')
    parser.add_argument('--kitchen-tasks', type=str, default=None,
                        help='Kitchen tasks to complete (comma-separated, e.g., "microwave,kettle")')
    
    # Training
    parser.add_argument('--num-steps', type=int, default=1000001, metavar='N',
                        help='maximum number of steps (default: 1000000)')
    parser.add_argument('--eval-freq', type=int, default=10, metavar='N',
                        help='evaluation frequency in episodes (default: 10)')
    parser.add_argument('--start-steps', type=int, default=10000, metavar='N',
                        help='Steps sampling random actions (default: 10000)')
    parser.add_argument('--updates-per-step', type=int, default=1, metavar='N',
                        help='model updates per simulator step (default: 1)')
    parser.add_argument('--batch-size', type=int, default=256, metavar='N',
                        help='batch size (default: 256)')
    
    # Network
    parser.add_argument('--policy', default="Gaussian",
                        help='Policy Type: Gaussian | Deterministic (default: Gaussian)')
    parser.add_argument('--hidden-size', type=int, default=256, metavar='N',
                        help='hidden size (default: 256)')
    
    # SAC parameters
    parser.add_argument('--gamma', type=float, default=0.99, metavar='G',
                        help='discount factor for reward (default: 0.99)')
    parser.add_argument('--tau', type=float, default=0.005, metavar='G',
                        help='target smoothing coefficient(Ï„) (default: 0.005)')
    parser.add_argument('--lr', type=float, default=0.0003, metavar='G',
                        help='learning rate (default: 0.0003)')
    parser.add_argument('--alpha', type=float, default=0.2, metavar='G',
                        help='Temperature parameter Î± (default: 0.2)')
    parser.add_argument('--automatic-entropy-tuning', type=bool, default=False, metavar='G',
                        help='Automatically adjust Î± (default: False)')
    parser.add_argument('--target-update-interval', type=int, default=1, metavar='N',
                        help='Value target update per no. of updates per step (default: 1)')
    
    # Memory
    parser.add_argument('--replay-size', type=int, default=1000000, metavar='N',
                        help='size of replay buffer (default: 1000000)')
    
    # W2 regularization parameters
    parser.add_argument('--w2-reg-weight', type=float, default=0.1, metavar='G',
                        help='W2 regularization weight (default: 0.1)')
    parser.add_argument('--old-policy-update-freq', type=int, default=5, metavar='N',
                        help='old policy update frequency (default: 5)')
    
    # Multi-step actor parameters
    parser.add_argument('--num-actors', type=int, default=1, metavar='N',
                        help='number of actors for multi-step update (default: 1)')
    parser.add_argument('--w2-weights', type=str, default=None, metavar='G',
                        help='W2 weights for each actor (comma-separated, e.g., "0.0,0.1")')
    
    # System
    parser.add_argument('--cuda', action="store_true",
                        help='run on CUDA (default: False)')
    
    # Output
    parser.add_argument('--checkpoint-dir', type=str, default="./checkpoints",
                        help='checkpoint directory (default: ./checkpoints)')
    parser.add_argument('--results-dir', type=str, default="./results",
                        help='results directory (default: ./results)')
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    # ì‹¤í—˜ í™˜ê²½ ì •ë³´ ì¶œë ¥
    print("=" * 60)
    print("SAC ì‹¤í—˜ ì„¤ì •")
    print("=" * 60)
    print(f"Environment: {args.env_name}")
    print(f"Seed: {args.seed}")
    print(f"Max timesteps: {args.num_steps:,}")
    print(f"Evaluation frequency: {args.eval_freq} episodes")
    print(f"Number of actors: {args.num_actors}")
    print(f"W2 reg weight: {args.w2_reg_weight}")
    print(f"Learning rate: {args.lr}")
    print(f"Batch size: {args.batch_size}")
    print(f"Discount: {args.gamma}")
    print(f"Tau: {args.tau}")
    print(f"Alpha: {args.alpha}")
    print(f"Automatic entropy tuning: {args.automatic_entropy_tuning}")
    print("=" * 60)
    print()
    
    # Parse w2_weights
    if args.w2_weights is not None:
        try:
            w2_weights_list = [float(x.strip()) for x in args.w2_weights.split(',')]
            args.w2_weights = w2_weights_list
        except ValueError:
            try:
                single_value = float(args.w2_weights)
                args.w2_weights = [0.0] + [single_value] * (args.num_actors - 1)
            except ValueError:
                print(f"Warning: Invalid w2_weights format '{args.w2_weights}', using default [0.0]")
                args.w2_weights = [0.0] * args.num_actors
    else:
        args.w2_weights = [0.0] + [args.w2_reg_weight] * (args.num_actors - 1)
    
    print(f"W2 weights: {args.w2_weights}")
    print()
    
    # í™˜ê²½ ì„¤ì • (ì‹œë“œ ì™„ì „íˆ ê³ ì •)
    # Kitchen í™˜ê²½ì˜ ê²½ìš° tasks_to_complete ì„¤ì •
    if 'Kitchen' in args.env_name and args.kitchen_tasks is not None:
        tasks = [t.strip() for t in args.kitchen_tasks.split(',')]
        env = gym.make(args.env_name, tasks_to_complete=tasks)
    else:
        env = gym.make(args.env_name)
    set_global_seed(env, args.seed)
    # ì¶”ê°€ ì‹œë“œ ê³ ì • (ëª¨ë“  ëœë¤ ì†ŒìŠ¤)
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    import random
    random.seed(args.seed)
    
    # Observation space ì²˜ë¦¬ (Dict observation space ì§€ì›)
    if isinstance(env.observation_space, gym.spaces.Dict):
        # Dict observation spaceì˜ ê²½ìš° 'observation' í‚¤ ì‚¬ìš©
        obs_space = env.observation_space['observation']
        obs_dim = obs_space.shape[0]
    else:
        obs_dim = env.observation_space.shape[0]
    
    # Agent ìƒì„± (ì‹œë“œ ì „ë‹¬í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”ë„ ê³ ì •)
    agent = SAC(obs_dim, env.action_space, args)
    
    # íŒŒì¼ ì´ë¦„ ë° ë””ë ‰í† ë¦¬ ì„¤ì •
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    agent_name = "POSAC" if args.w2_reg_weight > 0 else "SAC"
    # w2_weightsë¥¼ íŒŒì¼ëª…ì— í¬í•¨í•˜ì—¬ êµ¬ë¶„ (ì ì„ pë¡œ ë³€ê²½)
    w2_str = "_".join([str(w).replace(".", "p") for w in args.w2_weights[:3]])
    file_name = f"{agent_name}_{args.env_name}_{args.seed}_w2_{w2_str}_{timestamp}"
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    ckpt_dir = Path(args.checkpoint_dir)
    results_dir = Path(args.results_dir) / file_name
    results_dir.mkdir(parents=True, exist_ok=True)
    ckpt_dir.mkdir(parents=True, exist_ok=True)
    
    # TensorBoard
    writer = SummaryWriter(f'runs/{file_name}')
    
    # Memory
    memory = ReplayMemory(args.replay_size, args.seed)
    
    # í•™ìŠµ
    agent, episode_rewards = train_unified(
        agent, env, args.env_name, args.seed, memory,
        max_steps=args.num_steps,
        eval_freq=args.eval_freq,
        updates_per_step=args.updates_per_step,
        start_steps=args.start_steps,
        batch_size=args.batch_size,
        file_name=file_name,
        ckpt_dir=str(ckpt_dir),
        results_dir=results_dir,
        writer=writer,
        start_step=0
    )
    
    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    save_checkpoint(agent, args.env_name, str(ckpt_dir), f"{file_name}_final", args.num_steps, {
        "file_name": file_name,
        "episode_rewards": episode_rewards[-100:] if len(episode_rewards) > 100 else episode_rewards
    })
    
    # ìµœì¢… í‰ê°€
    num_actors = getattr(agent, "num_actors", 1)
    print("\n======== Final Evaluation (all actors) ========")
    for i in range(num_actors):
        print(f"\n======== Final Evaluation: Actor {i} ========")
        final_evaluation(
            agent, args.env_name, args.seed,
            runs=5, episodes=10, actor_idx=i
        )
    
    # ê²°ê³¼ ì €ì¥
    final_data = {
        'agent': agent_name,
        'episode_rewards': episode_rewards,
        'config': {
            'env_name': args.env_name,
            'seed': args.seed,
            'num_actors': args.num_actors,
            'w2_weights': args.w2_weights,
            'w2_reg_weight': args.w2_reg_weight,
        }
    }
    
    with open(results_dir / "results.json", 'w') as f:
        json.dump(final_data, f, indent=2)
    
    np.save(results_dir / "episode_rewards.npy", episode_rewards)
    
    print(f"\nâœ… ê²°ê³¼ê°€ {results_dir}/ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    env.close()


if __name__ == "__main__":
    main()
